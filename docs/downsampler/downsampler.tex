\documentclass[a4paper,11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{microtype}
\usepackage{algpseudocode, algorithm}
\usepackage{hyperref}
\usepackage{physics}

\title{Downsampling inelastic cyclic stress-strain data with minimal points to preserve curvature}
\author{A Hartloper}

\begin{document}
\maketitle

\begin{abstract}
    A downsampling algorithm for stress-strain data that uses few points but retains curvature within a pre-defined tolerance is developed in this document.
\end{abstract}

\section{Introduction}

The raw data (i.e., extensometer displacement, load cell force) in experiments used for material model calibration are typically sampled at 10~Hz in the EPFL test setup.
As experiments range from around a few minutes (e.g., LP1) to hours (LP5), this results in thousands to hundreds of thousands of data points for each experiment.
However, the sampled data is often denser than required to reasonably describe the stress-strain behavior of materials under the applied strain rates.
When it comes to calibrating material models, the time required is proportional to the number of data points.
Therefore, time can be saved by reducing the number of data points prior to running the calibration while maintaining the fidelity of the data.

Experience has shown that the fidelity of an experiment can be kept with around 10--100 times less points depending on the load protocol.
Calibrating the UVC model with ten load protocols \emph{without} reducing the number of points is, therefore, expected to take around several weeks to a year.
A ``good'' method of reducing the number of data points, or \emph{downsampling} the data, is essential to keep the calibration time reasonable (say, on the order of days to a week per material).
The properties that define a good downsampling method in this context are now defined.

A good downsampler for inelastic, cyclic, stress-strain data of structural steels:
\begin{enumerate}
    \item samples enough points in the initial elastic region to obtain a good estimate of the initial elastic modulus,
    \item samples the upper yield point,
    \item samples all the maxima and minima in each loading cycle (i.e., the \emph{peaks} of the stress-strain graph),
    \item samples enough points to retain the fidelity of regions in the stress-strain graph with high curvature,
    \item samples as few points as possible in regions of low curvature,
    \item can handle different load protocols,
    \item can handle noise in the measurements,
    \item can handle different strain rates,
    \item has interpretable heuristic parameters,
    \item is easy to use.
\end{enumerate}
An algorithm that combines different techniques to satisfy these desirable properties is developed in this document.

\section{Proposed algorithm}

The proposed algorithm is composed of three parts.
The first part is to find the peaks in the stress-strain data.
The second part is to downsample the entire stress-strain history based on the curvature of the stress-strain data.
The third part is to keep additional points in the initial elastic region.
The final downsampled data is the unique set combining all three parts.

Mosts of the heavy lifting is done in the second part.
The first part is essential to overcome \emph{aliasing} in the sampled data.
The third part is essential to ensure fidelity of the initial elastic region.
Details are given for each part and a summary of the entire procedure is given afterwards.


\subsection{Identifying peaks in the stress-strain graph}

The peaks in the stress-strain graph are identified by the minima or maxima in each cycle.
Cycles can be fairly reliably identified by the switching of signs in the stress signal.
Therefore, the peaks are the minima and maxima between each change in sign of the stress signal.
This method works well if the stress crosses zero in each cycle and there is no substantial noise in the stress signal around zero.

An attempt at identifying upper yield point, if it exists, is made in this first part.
First, the 0.2~\% offset yield stress is computed using the initial elastic modulus.
The initial elastic modulus itself is computed using linear regression up to $0.65 f_{y,n}$, where $f_{y,n}$ is the nominal yield stress of the material.
Then, the upper yield point is selected as the point of maximum stress up to the 0.2~\% offset yield stress.
If there is no upper yield point (due to the material or imperfections in the specimen), then the maximum will typically be at the 0.2~\% offset yield stress, and this point is sampled instead.

Another important point in stress-strain data tested according to the RESSLab protocols is the first instance 2~\% strain amplitude is crossed.
This point is important because the strain rate changes by a factor of around 30 at this point.
Therefore, this point is identified as well.
The final point of importance is the final point.
Unless otherwise specified, the final point is selected at 12.5~\% strain due to limitations in the small extensometer used for M8 experiments.
Any points past the final point are neglected.

To summarize, the sampled points from the first part are:
\begin{itemize}
    \item the upper yield point (or 0.2~\% offset yield stress point),
    \item the point where 2~\% strain amplitude is crossed (not applicable for LPs~4 and~5),
    \item the peaks in the stress-strain graph, and
    \item the final strain point (typically 12.5~\% strain).
\end{itemize}

\subsection{Curvature-based downsampling}

The idea in the second part of the overall procedure is to remove as many points as possible in the ``straight'' regions of the stress-strain graph and keep as many as necessary in the ``curved'' regions.
The algorithm used to accomplish this task was based on \url{https://kaushikghose.wordpress.com/2017/11/25/adaptively-downsampling-a-curve/}.
The key to this algorithm is to define what is ``straight'', and to sample a point every time that this condition is violated.

The method, named the ``maximum deviation downsampler'', is outlined in Algorithm~\ref{alg:max-dev-downsampler}.
This method requires a set of points and a tolerance, $\epsilon$.
The algorithm starts at the first point and steps through each point in the set.
A line is computed between the starting point and the current point.
The perpendicular distance is computed between the line and each point in the range of the starting and current point.
If the perpendicular distance is greater than the tolerance, the point before the current point is sampled; the sampled point is then set as the starting point.

The keys to making Algorithm~\ref{alg:max-dev-downsampler} in the current context are to: compute an appropriate perpendicular distance, to use an appropriate tolerance, and to remove noise from the data.
One challenge in stress-strain data is that the ``distances'' in stress and strain have different units.
This is handled by normalizing the stress data by the range in stress ($\sigma_i^* = \sigma_i / (\max \sigma - \min \sigma)$), and the strain by the range in strain ($\varepsilon_i^* = \varepsilon_i / (\max \varepsilon - \min \varepsilon)$).
Therefore, the normalized values have a range of 1.0 in both axes.

Given this normalization, a tolerance of $\epsilon = 0.001$ has been found to be appropriate.
This tolerance can be interpreted that ``straight'' is defined as 1/1000'th of the maximum ``distance'' spanned by the graph.
Finally, noise in the data can cause spurious sampling of points.
Therefore, a moving average filter is applied to the stress-strain data prior to normalization.
The moving average filter requires a window length and an interpolation order.
A window length of 55 is used for the data prior to 2~\% strain (low strain rate), and a window length of 5 is used for the data past 2~\% strain.

Filtering the stress-strain data removes the noise, but leads to aliasing.
Recall that the peaks of the stress-strain signal are already included in part 1 of the overall procedure, therefore, aliasing is not an issue in part 2.
For this reason, linear interpolation is used because it is most effective at reducing the noise (the trade-off is that it increases aliasing---which we have already dealt with).
Therefore, the set of 2-D points $\{x_i\}$ input to Algorithm~\ref{alg:max-dev-downsampler} are the filtered, normalized stress-strain data.

\begin{algorithm}
	\caption{Maximum deviation downsampler.}
	\label{alg:max-dev-downsampler}
	\begin{algorithmic}[1]
		\State \textbf{input:} Set of points, $\{x_i\}_{i=0}^{N-1}$ with $x_i \in \mathbb{R}^n$ and $N \in \mathbb{Z}$; and tolerance, $\epsilon \in \mathbb{R}$.
        \State \textbf{output:} Set of indices between $[0, N)$ to keep, $k_{sample}$.
        \State $k_{sample} \gets \emptyset$
        \State $i \gets 0$
		\For{$j \in (0, \ldots, N)$}
			\State $l \gets x_j - x_i$
            \Comment Line between $i$ and $j$
            \For{$k \in (i, \ldots, j)$}
                \State $d_k \gets x_k \perp l$
                \Comment Perpendicular line between $x_k$ and $l$
            \EndFor
            \If{$\max \norm{d_k}_2 > \epsilon$}
                \State $k_{sample} \gets k_{sample} \cup \{k - 1 \}$
                \State $i \gets k - 1$
            \EndIf
		\EndFor
        \State \textbf{return:} 
\end{algorithmic}
\end{algorithm}

\subsection{Overall algorithm}


\begin{algorithm}
	\caption{Overall summary of the proposed downsampling method.}
	\label{alg:overall-summary}
	\begin{algorithmic}[1]
        \State hi
    \end{algorithmic}
\end{algorithm}

\end{document}

